{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, Sequential","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:15.760485Z","iopub.execute_input":"2021-12-11T18:47:15.761035Z","iopub.status.idle":"2021-12-11T18:47:25.141117Z","shell.execute_reply.started":"2021-12-11T18:47:15.760983Z","shell.execute_reply":"2021-12-11T18:47:25.140141Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"def decontract(text):\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:25.143209Z","iopub.execute_input":"2021-12-11T18:47:25.143535Z","iopub.status.idle":"2021-12-11T18:47:25.160935Z","shell.execute_reply.started":"2021-12-11T18:47:25.143482Z","shell.execute_reply":"2021-12-11T18:47:25.159539Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nnegative_stop_words = set(word for word in stop_words if \"n't\" in word or 'no' in word)\nstop_words = stop_words - negative_stop_words","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:25.163293Z","iopub.execute_input":"2021-12-11T18:47:25.163699Z","iopub.status.idle":"2021-12-11T18:47:25.185663Z","shell.execute_reply.started":"2021-12-11T18:47:25.163635Z","shell.execute_reply":"2021-12-11T18:47:25.184096Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef preprocess_text(review):\n    review = re.sub(r\"http\\S+\", \"\", review)             # removing website links\n    review = BeautifulSoup(review, 'lxml').get_text()   # removing html tags\n    review = decontract(review)                         # decontracting\n    review = re.sub(\"\\S*\\d\\S*\", \"\", review).strip()     # removing the words with numeric digits\n    review = re.sub('[^A-Za-z]+', ' ', review)          # removing non-word characters\n    review = review.lower()                             # converting to lower case\n    review = [word for word in review.split(\" \") if not word in stop_words] # removing stop words\n    review = [lemmatizer.lemmatize(token, \"v\") for token in review] #lemmatization\n    review = \" \".join(review)\n    review.strip()\n    return review","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:25.188115Z","iopub.execute_input":"2021-12-11T18:47:25.189395Z","iopub.status.idle":"2021-12-11T18:47:25.200761Z","shell.execute_reply.started":"2021-12-11T18:47:25.189281Z","shell.execute_reply":"2021-12-11T18:47:25.199934Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import keras\nmodel = keras.models.load_model('/kaggle/input/model64/model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:25.205541Z","iopub.execute_input":"2021-12-11T18:47:25.205825Z","iopub.status.idle":"2021-12-11T18:47:29.254693Z","shell.execute_reply.started":"2021-12-11T18:47:25.205779Z","shell.execute_reply":"2021-12-11T18:47:29.253663Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/tokenizer/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:29.256432Z","iopub.execute_input":"2021-12-11T18:47:29.256739Z","iopub.status.idle":"2021-12-11T18:47:29.478724Z","shell.execute_reply.started":"2021-12-11T18:47:29.256684Z","shell.execute_reply":"2021-12-11T18:47:29.477941Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_review_length = 100\ndef predecir(text):\n    data = {'Text': [text]}\n    df = pd.DataFrame(data)\n    df['Text'] = df['Text'].apply(lambda x: preprocess_text(x))\n    x = tokenizer.texts_to_sequences(df['Text'].iloc[:1])\n    xpredict = pad_sequences(x, maxlen=max_review_length)\n    prediction = model.predict_classes(xpredict)\n    return prediction[0] + 1","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:29.479886Z","iopub.execute_input":"2021-12-11T18:47:29.480298Z","iopub.status.idle":"2021-12-11T18:47:29.487549Z","shell.execute_reply.started":"2021-12-11T18:47:29.480254Z","shell.execute_reply":"2021-12-11T18:47:29.486715Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:47:29.488679Z","iopub.execute_input":"2021-12-11T18:47:29.489111Z","iopub.status.idle":"2021-12-11T18:47:29.504270Z","shell.execute_reply.started":"2021-12-11T18:47:29.489067Z","shell.execute_reply":"2021-12-11T18:47:29.503370Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 100, 100)          9784700   \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 200)               160800    \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 1005      \n=================================================================\nTotal params: 9,946,505\nTrainable params: 9,946,505\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"text = input()\npredecir(text)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T18:48:34.758222Z","iopub.execute_input":"2021-12-11T18:48:34.758894Z","iopub.status.idle":"2021-12-11T18:48:35.851429Z","shell.execute_reply.started":"2021-12-11T18:48:34.758829Z","shell.execute_reply":"2021-12-11T18:48:35.850425Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdin","text":" I bought this to save money by cutting/slicing my own foods. I make my own bread and this enables me to have lovely thin sliced bread I buy my ham and cheese uncut/shaved/shredded and this enables me to slice/shave my own ham and cheese. Good money saver while not restricting myself on how I want my food prepared.\n"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]}]}